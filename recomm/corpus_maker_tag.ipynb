{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 1안\n",
    "# 영문: 토큰화, 모든 단어를 기본형으로 변환, 유의미 품사 추출\n",
    "# 한글: 형태소 분석, 유의미 형태소 추출\n",
    "# 각 단어별로 장소 임베딩\n",
    "# # 2안\n",
    "# word2vec으로 모든 단어를 임베딩한 후,\n",
    "# k-means clustering으로 군집화하여 군집별로 장소 임베딩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import platform\n",
    "import sys\n",
    "import time\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "nltk.data.path.append('F:\\\\소공전프로젝트\\\\sofcon\\\\recomm\\\\nltk_data')\n",
    "list_csv = ['data/kor/attraction_review_tag.csv',\n",
    "            'data/kor/hotel_review_tag.csv',\n",
    "            'data/kor/restaurant_review_tag.csv',\n",
    "            'data/eng/eng_attraction_review_tag.csv',\n",
    "            'data/eng/eng_hotel_review_tag.csv',\n",
    "            'data/eng/eng_restaurant_review_tag.csv']\n",
    "list_corpus = ['corpus/attraction_tag.list',\n",
    "               'corpus/hotel_tag.list',\n",
    "               'corpus/restaurant_tag.list']\n",
    "try:\n",
    "    os.stat('corpus')\n",
    "except:\n",
    "    os.mkdir('corpus')\n",
    "def orderset(seq):\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73789"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(list_csv[2])\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한글"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if platform.system() == 'Linux':\n",
    "    mecab = Mecab()\n",
    "elif platform.system() == 'Windows':\n",
    "    mecab = Mecab(dicpath=\"C:\\\\mecab\\\\mecab-ko-dic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus 생성함수\n",
    "def mkcorpus(ws):\n",
    "    for word in ws :\n",
    "        places = []\n",
    "        for i in range(len(df_morpheme)):\n",
    "            if word in df_morpheme['tags'][i]:\n",
    "                if not df_morpheme['placeId'][i] in places:\n",
    "                    places.append(df_morpheme['placeId'][i])\n",
    "        corpus.append(places)\n",
    "        #print('['+word+']: ',len(places),' places appended to the corpus')\n",
    "        #sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 246208\n",
      "집합 13226\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "for csv in range(3):\n",
    "    df = pd.read_csv(list_csv[csv])\n",
    "    # filter charset exception\n",
    "    df['review'] = df['review'].apply(lambda x: re.sub(r'[^ 가-힣0-9.!?\\n]',' ',x))\n",
    "    # make sentence list\n",
    "    array = df['review'].tolist()\n",
    "    # 한글형태소 분리\n",
    "    list_pos = [mecab.pos(sentence) for sentence in array]\n",
    "    # 형태소 리스트화\n",
    "    morpheme = [mecab.morphs(sentence) for sentence in array]\n",
    "\n",
    "    # 의미를 가지는 형태소만 추출\n",
    "    pattern = re.compile('MM|NNG|VA[+].*|VV[+].*|XR')\n",
    "    df_morpheme = pd.DataFrame(columns = ['placeId','tags'], dtype = 'int64')\n",
    "    taglist = []\n",
    "    for i in range(len(list_pos)):\n",
    "        pairs = np.array(list_pos[i])\n",
    "        tags = np.array(morpheme[i])\n",
    "        npbool = []\n",
    "        for pair in pairs:\n",
    "            npbool.append(re.fullmatch(pattern,pair[1])!=None)\n",
    "        tags = tags[npbool]\n",
    "        taglist.append(tags.tolist())\n",
    "    df_morpheme['tags'] = taglist\n",
    "    df_morpheme['placeId'] = df['placeId'].astype('int64')\n",
    "\n",
    "    wordlist = []\n",
    "    for l in df_morpheme['tags']:\n",
    "        wordlist += l\n",
    "    wordset = orderset(wordlist)\n",
    "    print('In ',list_csv[csv])\n",
    "    print('단어전체', len(wordlist))\n",
    "    print('단어집합', len(wordset))\n",
    "    # 병렬처리를 위한 데이터 분할 \n",
    "    core_count = mp.cpu_count()\n",
    "    wordsubset = np.array_split(wordset, core_count)\n",
    "    # 멀티프로세스 연산\n",
    "    if __name__ == '__main__':\n",
    "        start = time.time()\n",
    "\n",
    "        corpus = []\n",
    "        pool = Pool(core_count)\n",
    "        pool.map(mkcorpus, wordsubset)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "        print('Elapsed time: ', str(time.time() - start))\n",
    "        # save\n",
    "        with open(list_corpus[csv],'wb') as f:\n",
    "            pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 영문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos태깅은 문장이 문맥을 구성해야 정확한 결과가 나옴\n",
    "s = 'the quick brown fox jumps over the lazy dog'\n",
    "token = nltk.word_tokenize(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "st = PorterStemmer()\n",
    "lm = WordNetLemmatizer()\n",
    "l = ['eat', 'ate', 'eating', 'eaten']\n",
    "l2 = [st.stem(w) for w in l]\n",
    "print(l2)\n",
    "l3 = [lm.lemmatize(w, pos='v') for w in l2]\n",
    "print(l3)\n",
    "l4 = ['i', 'ate', 'a', 'grass','eating', 'chicken']\n",
    "nltk.pos_tag(l4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(list_csv[3])\n",
    "# filter charset exception\n",
    "df['review'] = df['review'].apply(lambda x: re.sub(r'[^ a-zA-Z0-9.!?\\n]',' ',x))\n",
    "# make sentence list\n",
    "array = df['review'].tolist()\n",
    "array"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
